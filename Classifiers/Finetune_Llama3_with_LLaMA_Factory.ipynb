{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetune Llama-3 with LLaMA Factory\n",
        "\n",
        "Please use a **free** Tesla T4 Colab GPU to run this!\n",
        "\n",
        "Project homepage: https://github.com/hiyouga/LLaMA-Factory"
      ],
      "metadata": {
        "id": "1oHFCsV0z-Jw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "lr7rB3szzhtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uv68RtGNYdgA",
        "outputId": "8aecc6f3-f3f3-47c4-d69a-c0e27ae5f2e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giM74oK1rRIH"
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n",
        "!pip uninstall -y jax\n",
        "!pip install -e .[torch,bitsandbytes,liger-kernel]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp path/to/dataset_info.json /content/LLaMA-Factory/data"
      ],
      "metadata": {
        "id": "myiz-uDywyv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/LLaMA-Factory/data"
      ],
      "metadata": {
        "id": "FI1xXwWm8Dgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open(\"path/to/train/articles.json\", \"r\") as f:\n",
        "  train_data = json.load(f)\n",
        "with open(\"path/to/train/articles.json\", \"r\") as f2:\n",
        "  test_data = json.load(f2)"
      ],
      "metadata": {
        "id": "SFL756lMwUeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = 3"
      ],
      "metadata": {
        "id": "_izkWedk2mUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'Responda apenas com sim ou nao, se você considera esse texto como sendo de autoria humana: ' if NUM_CLASSES == 2 else 'Respondendo apenas com 0, 1 ou 2, responda se você considera esse texto como sendo de autoria humana, autoria articial ou autoria humana reescrito por inteligência artificial: '"
      ],
      "metadata": {
        "id": "jHz7Cz0V1X4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def makeAlpaca3Classes(train_data):\n",
        "  alpaca = []\n",
        "  for article in train_data:\n",
        "    alpaca.append({'instruction': question + article['text'], 'input': '', 'output': str(article['class_label'])})\n",
        "  json.dump(alpaca, open(\"/content/LLaMA-Factory/data/Alpaca.json\", \"w\", encoding=\"utf-8\"),ensure_ascii=False, indent=2)"
      ],
      "metadata": {
        "id": "24MMBTMcUq7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def makeAlpaca2Classes(train_data):\n",
        "  alpaca = []\n",
        "  for article in train_data:\n",
        "    alpaca.append({'instruction': question + article['text'], 'input': '', 'output': 'sim' if article['class_label'] == 0 else 'não'})\n",
        "  json.dump(alpaca, open(\"/content/LLaMA-Factory/data/Alpaca.json\", \"w\", encoding=\"utf-8\"),ensure_ascii=False, indent=2)"
      ],
      "metadata": {
        "id": "V0DaOkD8ypaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "makeAlpaca3Classes(train_data)"
      ],
      "metadata": {
        "id": "XHXBe7ML8n-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "makeAlpaca2Classes(train_data)"
      ],
      "metadata": {
        "id": "z-8cTW5Jd_FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune model via LLaMA Board"
      ],
      "metadata": {
        "id": "2QiXcvdzzW3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "O4IWWW7b6hl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLaMA-Factory/\n",
        "!GRADIO_SHARE=1 llamafactory-cli webui"
      ],
      "metadata": {
        "id": "YLsdS6V5yUMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune model via Command Line\n",
        "\n",
        "It takes ~30min for training."
      ],
      "metadata": {
        "id": "rgR3UFhB0Ifq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  stage=\"sft\",                        # do supervised fine-tuning\n",
        "  do_train=True,\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "  dataset=\"identity,alpaca_en_demo\",             # use alpaca and identity datasets\n",
        "  template=\"llama3\",                     # use llama3 prompt template\n",
        "  finetuning_type=\"lora\",                   # use LoRA adapters to save memory\n",
        "  lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n",
        "  output_dir=\"llama3_lora\",                  # the path to save LoRA adapters\n",
        "  per_device_train_batch_size=2,               # the batch size\n",
        "  gradient_accumulation_steps=4,               # the gradient accumulation steps\n",
        "  lr_scheduler_type=\"cosine\",                 # use cosine learning rate scheduler\n",
        "  logging_steps=10,                      # log every 10 steps\n",
        "  warmup_ratio=0.1,                      # use warmup scheduler\n",
        "  save_steps=1000,                      # save checkpoint every 1000 steps\n",
        "  learning_rate=5e-5,                     # the learning rate\n",
        "  num_train_epochs=3.0,                    # the epochs of training\n",
        "  max_samples=500,                      # use 500 examples in each dataset\n",
        "  max_grad_norm=1.0,                     # clip gradient norm to 1.0\n",
        "  loraplus_lr_ratio=16.0,                   # use LoRA+ algorithm with lambda=16.0\n",
        "  fp16=True,                         # use float16 mixed precision training\n",
        "  use_liger_kernel=True,                   # use liger kernel for efficient training\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"train_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train train_llama3.json"
      ],
      "metadata": {
        "id": "CS0Qk5OR0i4Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf1de58f-317c-407c-c8ac-5b7eea085088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Infer the fine-tuned model"
      ],
      "metadata": {
        "id": "PVNaC-xS5N40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLaMA-Factory/src"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pgwCzzCZRER",
        "outputId": "14fe1e3c-5ccf-49b2-e916-abaa1189bbf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llamafactory.chat import ChatModel\n",
        "from llamafactory.extras.misc import torch_gc\n",
        "import time\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"meta-llama/Llama-3.1-8B-Instruct\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "  #adapter_name_or_path=\"/content/LLaMA-Factory/saves/Llama-3.2-3B-Instruct/lora/train1\",            # leave empty for 0-Shot classification\n",
        "  template=\"llama3\",\n",
        "  finetuning_type=\"lora\",\n",
        ")\n",
        "chat_model = ChatModel(args)"
      ],
      "metadata": {
        "id": "ABeAf8Oc0jc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "now = time.time()\n",
        "for article in test_data:\n",
        "  messages =[]\n",
        "  torch_gc()\n",
        "  print(\"History has been removed.\")\n",
        "  query = question + article['text']\n",
        "\n",
        "  messages.append({\"role\": \"user\", \"content\": query})\n",
        "  print(\"Assistant: \", end=\"\", flush=True)\n",
        "\n",
        "  response = \"\"\n",
        "  for new_text in chat_model.stream_chat(messages):\n",
        "    print(new_text, end=\"\", flush=True)\n",
        "    response += new_text\n",
        "  print()\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "  results.append({'title': article['title'], 'text': article['text'], 'class_label': article['class_label'], 'prediction': response})\n",
        "then = time.time()"
      ],
      "metadata": {
        "id": "oh8H9A_25SF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Time elapsed: {then - now} seconds\")"
      ],
      "metadata": {
        "id": "CFRdqI78g2MV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch_gc()\n",
        "print(f\"Time elapsed: {then - now} seconds\")\n",
        "json.dump(results, open(f\"/content/drive/MyDrive/Faculdade/Monografia/0Shot_Llama3_1{NUM_CLASSES}ClassesResults.json\", \"w\", encoding=\"utf-8\"), indent=2, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "KEvtostS0p3O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e0b2978-f904-4c12-af55-bc2832353ba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time elapsed: 250.71269965171814 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge the LoRA adapter and optionally upload model\n",
        "\n",
        "NOTE: the Colab free version has merely 12GB RAM, where merging LoRA of a 8B model needs at least 18GB RAM, thus you **cannot** perform it in the free version."
      ],
      "metadata": {
        "id": "kTESHaFvbNTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "mcNcHcA4bf4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"meta-llama/Meta-Llama-3-8B-Instruct\", # use official non-quantized Llama-3-8B-Instruct model\n",
        "  adapter_name_or_path=\"llama3_lora\",            # load the saved LoRA adapters\n",
        "  template=\"llama3\",                     # same to the one in training\n",
        "  finetuning_type=\"lora\",                  # same to the one in training\n",
        "  export_dir=\"llama3_lora_merged\",              # the path to save the merged model\n",
        "  export_size=2,                       # the file shard size (in GB) of the merged model\n",
        "  export_device=\"cpu\",                    # the device used in export, can be chosen from `cpu` and `cuda`\n",
        "  #export_hub_model_id=\"your_id/your_model\",         # the Hugging Face hub ID to upload model\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"merge_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli export merge_llama3.json"
      ],
      "metadata": {
        "id": "IMojogHbaOZF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}